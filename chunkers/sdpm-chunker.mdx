---
title: 'SDPMChunker'
description: 'Split text using Semantic Double-Pass Merging for improved context preservation'
icon: 'brain-circuit'
---

# SDPMChunker

The `SDPMChunker` extends semantic chunking by using a double-pass merging approach. It first groups content by semantic similarity, then merges similar groups within a skip window, allowing it to connect related content that may not be consecutive in the text.

## Installation

SDPMChunker requires the same dependencies as SemanticChunker:

```bash
# Install with semantic features
pip install "chonkie[semantic]"

# Or install everything
pip install "chonkie[all]"
```

## Initialization

```python
from chonkie import SDPMChunker

# Basic initialization with default parameters
chunker = SDPMChunker(
    embedding_model="minishlab/potion-base-8M",  # Default model
    similarity_threshold=0.5,          # Similarity threshold (0-1)
    max_chunk_size=512,               # Maximum tokens per chunk
    initial_sentences=1,              # Initial sentences per chunk
    skip_window=1                     # Number of chunks to skip when looking for similarities
)

# Using similarity percentile instead of threshold
chunker = SDPMChunker(
    embedding_model="minishlab/potion-base-8M",
    similarity_percentile=80,         # Use 80th percentile as threshold
    max_chunk_size=512,
    skip_window=2                     # Look two chunks ahead for similarities
)

# Using a custom embedding model
from chonkie.embeddings import BaseEmbeddings

class CustomEmbeddings(BaseEmbeddings):
    # Implement required methods...
    pass

custom_embeddings = CustomEmbeddings()
chunker = SDPMChunker(
    embedding_model=custom_embeddings,
    similarity_threshold=0.5,
    max_chunk_size=512
)
```

## Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `embedding_model` | `Union[str, BaseEmbeddings]` | `"minishlab/potion-base-8M"` | Model identifier or embedding model instance |
| `similarity_threshold` | `Optional[float]` | `None` | Minimum similarity score (0-1) to consider sentences similar |
| `similarity_percentile` | `Optional[float]` | `0.8` if neither specified | Percentile-based threshold (0-100) for similarity |
| `max_chunk_size` | `int` | `512` | Maximum tokens per chunk |
| `initial_sentences` | `int` | `1` | Number of sentences to start each chunk with |
| `skip_window` | `int` | `1` | Number of chunks to skip when looking for similarities |

**Notes**: 
- Either `similarity_threshold` or `similarity_percentile` must be specified. If neither is specified, it defaults to 80th percentile with a warning.
- The `skip_window` parameter controls how far ahead the chunker looks for similar content.

## Return Type

SDPMChunker returns the same optimized `SemanticChunk` objects as SemanticChunker:

```python
@dataclass
class SemanticSentence(Sentence):
    text: str
    start_index: int
    end_index: int
    token_count: int
    embedding: Optional[np.ndarray]
    
    __slots__ = ['embedding']

@dataclass
class SemanticChunk(SentenceChunk):
    text: str
    start_index: int
    end_index: int
    token_count: int
    sentences: List[SemanticSentence]
```

## Usage

### Single Text Chunking

```python
# Example text with related but non-consecutive paragraphs
text = """
The neural network processes input data through layers.
Training data is essential for model performance.
GPUs accelerate neural network computations significantly.
Quality training data improves model accuracy.
TPUs provide specialized hardware for deep learning.
Data preprocessing is a crucial step in training.
"""

chunks = chunker.chunk(text)

for chunk in chunks:
    print(f"Chunk text: {chunk.text}")
    print(f"Token count: {chunk.token_count}")
    print(f"Number of sentences: {len(chunk.sentences)}")
```

### Batch Chunking

```python
texts = [
    "Document with scattered but related content...",
    "Another document with similar patterns..."
]
batch_chunks = chunker.chunk_batch(texts)

for doc_chunks in batch_chunks:
    for chunk in doc_chunks:
        print(f"Chunk: {chunk.text}")
```

## Performance Notes

SDPMChunker includes the same optimizations as SemanticChunker, plus:
- Efficient skip-window processing
- Smart group merging strategy
- Reuses computed embeddings across passes
- Memory-efficient dataclass implementation

## Best Practices

1. **Tuning the Skip Window:**
   - Start with `skip_window=1`
   - Increase for content with distant relationships
   - Consider content structure when setting
   - Monitor chunk coherence and size

2. **Similarity Thresholds:**
   - Use percentile-based threshold for unknown content
   - Higher thresholds create stricter grouping
   - Lower thresholds allow more merging
   - Balance with skip_window size

3. **Memory and Performance:**
   - Larger skip windows increase computation time
   - Monitor memory with large documents
   - Consider batch size for large datasets
   - Pre-process text to remove noise

4. **Content Types:**
   - Academic papers: Higher skip_window for related concepts
   - Technical docs: Balance skip_window with section size
   - Articles: Moderate skip_window for theme consistency
   - Code/API docs: Lower skip_window for local context

## When to Use SDPMChunker

SDPMChunker is particularly useful for:
- Documents with related content spread apart
- Technical documentation with recurring themes
- Academic papers with interconnected concepts
- Any content where local context isn't sufficient

Consider using SDPMChunker when:
- Regular semantic chunking misses relationships
- Content has themes that repeat
- You need to connect distant but related content
- Context preservation is crucial

Compare with SemanticChunker when:
- Processing speed is critical
- Content is already well-structured
- Local context is sufficient
- Memory usage is a concern