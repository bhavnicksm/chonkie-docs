---
title: 'TokenChunker'
description: 'Split text into fixed-size token chunks with configurable overlap'
icon: 'scissors'
---

The `TokenChunker` splits text into chunks based on token count, ensuring each chunk stays within specified token limits.

## Installation

TokenChunker is included in the base installation:

```bash
pip install chonkie
```

## Initialization

```python
from chonkie import TokenChunker

# Basic initialization with default parameters
chunker = TokenChunker(
    tokenizer="gpt2",  # Supports string identifiers
    chunk_size=512,    # Maximum tokens per chunk
    chunk_overlap=128  # Overlap between chunks
)

# Using a custom tokenizer
from tokenizers import Tokenizer
custom_tokenizer = Tokenizer.from_pretrained("your-tokenizer")
chunker = TokenChunker(
    tokenizer=custom_tokenizer,
    chunk_size=512,
    chunk_overlap=128
)
```

## Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `tokenizer` | `Union[str, tokenizers.Tokenizer, tiktoken.Encoding]` | `"gpt2"` | Tokenizer to use. Can be a string identifier or a tokenizer instance |
| `chunk_size` | `int` | `512` | Maximum number of tokens per chunk |
| `chunk_overlap` | `Union[int, float]` | `128` | Number or percentage of overlapping tokens between chunks |

## Usage

### Single Text Chunking

```python
text = "Some long text that needs to be chunked into smaller pieces..."
chunks = chunker.chunk(text)

for chunk in chunks:
    print(f"Chunk text: {chunk.text}")
    print(f"Token count: {chunk.token_count}")
    print(f"Start index: {chunk.start_index}")
    print(f"End index: {chunk.end_index}")
```

### Batch Chunking

```python
texts = [
    "First document to chunk...",
    "Second document to chunk..."
]
batch_chunks = chunker.chunk_batch(texts)

for doc_chunks in batch_chunks:
    for chunk in doc_chunks:
        print(f"Chunk: {chunk.text}")
```

### Using as a Callable

```python
# Single text
chunks = chunker("Text to chunk...")

# Multiple texts
batch_chunks = chunker(["Text 1...", "Text 2..."])
```

## Supported Tokenizers

TokenChunker supports multiple tokenizer backends:

- **TikToken** (Recommended)
  ```python
  import tiktoken
  tokenizer = tiktoken.get_encoding("gpt2")
  ```

- **AutoTikTokenizer**
  ```python
  from autotiktokenizer import AutoTikTokenizer
  tokenizer = AutoTikTokenizer.from_pretrained("gpt2")
  ```

- **Hugging Face Tokenizers**
  ```python
  from tokenizers import Tokenizer
  tokenizer = Tokenizer.from_pretrained("gpt2")
  ```

- **Transformers**
  ```python
  from transformers import AutoTokenizer
  tokenizer = AutoTokenizer.from_pretrained("gpt2")
  ```

## Return Type

TokenChunker returns chunks as `Chunk` objects with the following attributes:

```python
@dataclass
class Chunk:
    text: str           # The chunk text
    start_index: int    # Starting position in original text
    end_index: int      # Ending position in original text
    token_count: int    # Number of tokens in chunk
```

## Thread Safety

TokenChunker is thread-safe and can be reused across multiple calls without reinitialization. However, when using batch processing, be mindful of CPU usage as it may utilize multiple threads for optimization.

## Performance Notes

- TokenChunker uses TikToken by default which is 3-6x faster than alternatives
- Batch processing automatically utilizes available CPU cores for better performance
- Default to using string identifiers for tokenizers when possible, as they're initialized efficiently
- For large-scale processing, consider using `chunk_batch()` instead of multiple `chunk()` calls

## Example: Advanced Usage

```python
# Initialize with percentage-based overlap
chunker = TokenChunker(
    tokenizer="gpt2",
    chunk_size=512,
    chunk_overlap=0.2  # 20% overlap between chunks
)

# Process a large document with custom batch size
with open("large_document.txt", "r") as f:
    texts = f.readlines()
    
chunks = chunker.chunk_batch(texts, batch_size=1000)

# Extract specific chunks based on token count
long_chunks = [
    chunk for chunk_list in chunks
    for chunk in chunk_list
    if chunk.token_count > 256
]
```

## Best Practices

1. **Choose appropriate chunk sizes:**
   - Consider your model's context window
   - Leave room for prompts and other context
   - Default of 512 works well for most cases

2. **Set overlap based on use case:**
   - Higher overlap for better context preservation
   - Lower overlap for efficiency
   - Use percentage-based overlap for adaptability

3. **Batch processing for large datasets:**
   - Use `chunk_batch()` for multiple texts
   - Monitor CPU usage when processing large batches
   - Consider using a smaller batch size if memory is constrained