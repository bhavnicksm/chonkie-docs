---
title: 'SemanticChunker'
description: 'Split text into chunks based on semantic similarity'
icon: 'brain'
---

# SemanticChunker

The `SemanticChunker` splits text based on semantic similarity, ensuring that related content stays together in the same chunk. This approach is particularly useful for RAG applications where context preservation is crucial.

## Installation

SemanticChunker requires additional dependencies:

```bash
# Install with semantic features
pip install "chonkie[semantic]"

# Install with individual embeddings support; for example, Model2Vec
pip install "chonkie[model2vec]"

# Or use with OpenAI embeddings
pip install "chonkie[openai]"
```

## Initialization

```python
from chonkie import SemanticChunker

# Basic initialization with default parameters
chunker = SemanticChunker(
    embedding_model="minishlab/potion-base-8M",  # Default model
    similarity_threshold=0.5,          # Similarity threshold (0-1)
    chunk_size=512,                   # Maximum tokens per chunk
    initial_sentences=1               # Initial sentences per chunk
)

# Using similarity percentile instead of threshold
chunker = SemanticChunker(
    embedding_model="minishlab/potion-base-8M",
    similarity_percentile=80,         # Use 80th percentile as threshold
    chunk_size=512,
    initial_sentences=1
)

# Using a custom embedding model
from chonkie.embeddings import BaseEmbeddings

class CustomEmbeddings(BaseEmbeddings):
    # Implement required methods...
    pass

custom_embeddings = CustomEmbeddings()
chunker = SemanticChunker(
    embedding_model=custom_embeddings,
    similarity_threshold=0.5,
    chunk_size=512
)
```

## Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `embedding_model` | `Union[str, BaseEmbeddings]` | `"minishlab/potion-base-8M"` | Model identifier or embedding model instance |
| `similarity_threshold` | `Optional[float]` | `None` | Minimum similarity score (0-1) to consider sentences similar |
| `similarity_percentile` | `Optional[float]` | `0.8` if neither specified | Percentile-based threshold (0-100) for similarity |
| `chunk_size` | `int` | `512` | Maximum tokens per chunk |
| `initial_sentences` | `int` | `1` | Number of sentences to start each chunk with |

**Notes**: 
- You must specify either `similarity_threshold` or `similarity_percentile`. If neither is specified, it defaults to 80th percentile with a warning.
- The default embedding model has changed to `minishlab/potion-base-8M` for better performance.

## Return Type

SemanticChunker returns `SemanticChunk` objects with optimized storage using slots:

```python
@dataclass
class SemanticSentence(Sentence):
    text: str
    start_index: int
    end_index: int
    token_count: int
    embedding: Optional[np.ndarray]  # Sentence embedding vector
    
    __slots__ = ['embedding']  # Optimized memory usage

@dataclass
class SemanticChunk(SentenceChunk):
    text: str
    start_index: int
    end_index: int
    token_count: int
    sentences: List[SemanticSentence]
```

## Usage

### Single Text Chunking

```python
text = """First paragraph about a specific topic.
Second paragraph continuing the same topic.
Third paragraph switching to a different topic.
Fourth paragraph expanding on the new topic."""

chunks = chunker.chunk(text)

for chunk in chunks:
    print(f"Chunk text: {chunk.text}")
    print(f"Token count: {chunk.token_count}")
    print(f"Number of sentences: {len(chunk.sentences)}")
```

### Batch Chunking

```python
texts = [
    "First document about topic A...",
    "Second document about topic B..."
]
batch_chunks = chunker.chunk_batch(texts)

for doc_chunks in batch_chunks:
    for chunk in doc_chunks:
        print(f"Chunk: {chunk.text}")
```

## Performance Notes

SemanticChunker includes several optimizations:
- Uses memory-efficient slot-based dataclasses
- Implements fast sentence splitting algorithm
- Pre-computes embeddings for sentences
- Uses weighted mean pooling for chunk embeddings
- Caches computed embeddings for reuse
- Efficient batch processing of sentences

## Embedding Models

SemanticChunker supports multiple embedding providers through Chonkie's embedding system:

1. **Model2Vec Embeddings** (Default)
   ```python
   chunker = SemanticChunker("minishlab/potion-base-8M")
   ```
   - Faster and lighter than alternatives
   - Provides good balance of speed and accuracy
   - ~500x faster than standard embedding models

2. **Custom Embeddings**
   ```python
   from chonkie.embeddings import BaseEmbeddings
   
   class CustomEmbeddings(BaseEmbeddings):
       def embed(self, text: str) -> np.ndarray:
           # Implementation
           pass
       
       def embed_batch(self, texts: List[str]) -> List[np.ndarray]:
           # Implementation
           pass
       
       # Implement other required methods...
   ```

## Best Practices

1. **Choosing Similarity Thresholds:**
   - Use percentile-based threshold when unsure about content
   - Default to 80th percentile for balanced grouping
   - Increase threshold for stricter semantic grouping

2. **Performance Optimization:**
   - Use Model2Vec embeddings for faster processing
   - Implement custom embeddings for specific needs
   - Monitor memory usage with large documents
   - Pre-process text to remove noise

3. **Content Handling:**
   - Adjust initial_sentences based on content type
   - Consider sentence length and complexity
   - Test with representative content samples

4. **Model Selection:**
   - Start with default Model2Vec embeddings
   - Use domain-specific models if needed
   - Consider speed vs. accuracy trade-offs
   - Implement custom embeddings for special cases